---
title: "PSY 503: Foundations of Statistical Methods in Psychological Science"
subtitle: "More LM: Categorical Predictors"
institute: "Princeton University"
author: "Jason Geller, Ph.D. (he/him/his)"
date:  'Updated:`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
      background-image: url("lover.png")
      background-size: cover
      
---
```{r xaringan-extra-styles, echo=FALSE}
library(xaringanExtra)
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```

```{html, echo=FALSE}
<div style = "position:fixed; visibility: hidden">
$$\require{color}\definecolor{yellow}{rgb}{1, 0.8, 0.16078431372549}$$
$$\require{color}\definecolor{orange}{rgb}{0.96078431372549, 0.525490196078431, 0.203921568627451}$$
$$\require{color}\definecolor{green}{rgb}{0, 0.474509803921569, 0.396078431372549}$$
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      yellow: ["{\\color{yellow}{#1}}", 1],
      orange: ["{\\color{orange}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>
<style>
.yellow {color: #FFCC29;}
.orange {color: #F58634;}
.green {color: #007965;}
</style>
```{r, echo=FALSE}
library(flair)
yellow <- "#FFCC29"
orange <- "#F58634"
green <- "#007965"
```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "50%",
  tidy.opts=list(width.cutoff=60),tidy=TRUE, 
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  fig.show = TRUE,
  hiline = TRUE
)
hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})
```

<div style = "position:fixed; visibility: hidden">
$$\require{color}\definecolor{red}{rgb}{1, 0, 0}$$
$$\require{color}\definecolor{green}{rgb}{0, 1, 0}$$
$$\require{color}\definecolor{blue}{rgb}{0, 0, 1}$$
</div>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      red: ["{\\color{red}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1],
      blue: ["{\\color{blue}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>

<style>
.red {color: #FF0000;}
.green {color: #00FF00;}
.blue {color: #0000FF;}
</style>


```{r flair_color, echo=FALSE}
library(flair)
red <- "#FF0000"
green <- "#00FF00"
blue <- "#0000FF"
```


```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_solarized_dark(
  header_font_google = google_font("Work Sans"),
  header_h1_font_size = "36px",
  header_color = "black",
  text_font_google = google_font("Work Sans"),
  text_font_size = "32px",
  text_color = "black", 
  background_color = "white", 
  code_font_google = google_font("Share Tech Mono"),
  extra_css = list(
    ".remark-slide-content h2" = list(
      "margin-top" = "2em",
      "margin-bottom" = "2em"
    ),
    .big = list("font-size" = "150%"),
    .small = list("font-size" = "75%"),
    .subtle = list(opacity = "0.6"),
    ".countdown-has-style h3, .countdown-has-style h3 ~ p, .countdown-has-style h3 ~ ul" = list(
      "margin" = "0"
    ),
    ".countdown-has-style pre" = list(
      "margin-top" = "-10px"
    ),
    "p .remark-inline-code" = list(
      "background-color" = "white",
      "padding" = "2px 2px",
      "margin" = "0 -2px"
    ),
    blockquote = list("margin-left" = 0),
    "em" = list(color = "#2aa198")
  ),
)
```

```{r, echo=FALSE}
library(parameters)
library(effectsize) 
library(papaja)
library(tidyverse)
library(performance)
library(see)
library(equatiomatic)
library(kableExtra)
library(broom)
library(report)
library(flextable)
library(huxtable)
library(skimr)
library(papaja)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

master <- read_csv("https://raw.githubusercontent.com/jgeller112/psy503-psych_stats/master/static/slides/10-linear_modeling/data/regress.csv")

model2 <- lm(CESD_total~PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW, data=master)

apa_style <- apa_print(model2)

```
# Outline

- Check-in questions

- Effect size and power in MR

- Categorical predictors

  - How linear modeling is related to t-tests/ANOVAs
  
  - Contrast coding for categorical predictors with two means 
    
  - Contrast coding for categorical predictors with three means or more
---
# Check-in Questions

- What the heck is heteroskedasticity?

---
# Heteroskedasticity

- Non-constant error (residual) variance

  - Residuals and predictors are correlated

```{r, echo=FALSE, fig.align='center', out.width="60%"}

knitr::include_graphics("hetero.webp")

```

---
# Heteroskedasticity

- Consequences:

  - Causes standard errors (SE) to be unreliable
  
    - Increased Type 1 and Type 2 Error

- Solution:

  - Easiest would be to use robust methods for the SE     
---
.pull-left[
```{r}
library(performance)
model_parameters(model2) %>%
  flextable() %>%
  autofit()

```
]
.pull-right[
```{r}
model_parameters(model2, vcov = "HC3") %>% #get corrected SE# robust params
flextable() %>%
  autofit()# robust params
```
]
---
# Check-in Questions

- What the heck is heteroskedasticity?

- Assumptions of simple linear model

---
# Simple Linear Modeling Assumptions

- Normality of residuals

- Missingness

- Outliers

---
# Burning Questions

- What the heck is heteroskedasticity?

- Assumptions of simple linear model

- Difference between $\epsilon$ and residual
---
# Errors

- Error is basically an unobservable  quantity in our model

  - We use residuals as a proxy
---
# Outliers

```{r, echo = FALSE, out.width="70%", fig.align='center'}
knitr::include_graphics("out.png")
```
---
# Outliers: Discrepancy

> A data point that is unusual in the context of the least squares model 

$$e_i^* = \dfrac{e_i}{S_{E(-i)}\sqrt{1-h_i}}$$

```{r, echo = FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("hdl.png")
```
---
# Outliers: Leverage

> Leverage measures how far a data point is from the mean 

$$h_i = \dfrac{1}{n} + \dfrac{\left(x_i -\overline{x}\right)^2}{\sum_{j=1}^{n}{\left(x_j -\overline{x}\right)^2}} \qquad \text{and}\qquad \overline{h} = \dfrac{k}{n}  \qquad \text{and}\qquad \dfrac{1}{n} \leq h_i \leq 1.0$$
```{r, echo = FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("hdl.png")
```
---
# Cook's Distance

- A measure of how much of an effect a single data point has on the whole model 
  
- Often described as leverage + discrepancy (residuals)
  
$$D_i = \dfrac{e_i^2}{k \times \frac{1}{n}\sum{e_i^2}} \times \dfrac{h_i}{1-h_i}$$
$$e_i^* = \dfrac{e_i}{S_{E(-i)}\sqrt{1-h_i}}$$
- How do we calculate how much change is bad?
<p align="center">
    - $\frac{4}{N-K-1}$
---
# Calculate in R 

```{r, eval=FALSE}
augment(model) # in broom package

```

---
# The Impact of Individual Predictors on the Model: Effect Size

.pull-left[
- R is the multiple correlation
- $R^2$ is the multiple correlation squared
- All overlap in Y, used for overall model
- $A+B+C/(A+B+C+D)$
]

.pull-right[
```{r, echo = FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("https://raw.githubusercontent.com/doomlab/learnSTATS/master/vignettes/pictures/regression/19.PNG")
```
]
---
# Effect Size 

.pull-left[

- sr is the semi-partial correlations 

- Unique contribution of IV to $R^2$ for those IVs

- Increase in proportion of explained Y variance when X is added to the equation

- $A/(A+B+C+D)$
]

.pull-right[
```{r, echo = FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("https://raw.githubusercontent.com/doomlab/learnSTATS/master/vignettes/pictures/regression/19.PNG")
```
]
---
# Effect Size 

.pull-left[

- pr is the partial correlation

- Partial correlation asks how much of the Y variance, which is not estimated by the other IVs, is estimated by this variable.

- $A/(A+D)$

- Removes the shared variance of the control variable (Say X2) from both Y and X1

- Pr > sr
]

.pull-right[

```{r, echo = FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("https://raw.githubusercontent.com/doomlab/learnSTATS/master/vignettes/pictures/regression/19.PNG")
```
]
---
# Partial Correlations

- We would add these to our other reports:

    - Meaning: `r apa_style$full_result$PIL_total`, $pr^2 = .30$`
    - Alcohol: `r apa_style$full_result$AUDIT_TOTAL_NEW`, $pr^2 < .01$`
    - Drugs: `r apa_style$full_result$DAST_TOTAL_NEW`, $pr^2 < .01$`

```{r echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
library(ppcor)
partials <- pcor(master)
partials$estimate^2
#spcor.test (semi-partial)
```

---
# Multiple Regression: Power

- We can use the `pwr` library to calculate the required sample size for any particular effect size

- First, we need to convert the $R^2$ value to $f^2$, which is a different effect size, not the ANOVA *F*

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(pwr)
r2=glance(model2)

R2= r2$r.squared
f2 <- R2 / (1-R2)
R2
f2
```
---
# Multiple Regression: Power

- `u` is degrees of freedom for the model, first value in the F-statistic
- `v` is degrees of freedom for error, but we are trying to figure out sample size for each condition, so we leave this one blank. 
- `f2` is the converted effect size.
- `sig.level` is our $\alpha$ value
- `power` is our power level
- The final sample size is *v + k + 1 where k is the predictors*

```{r, eval=FALSE}
#f2 is cohen f squared 
pwr.f2.test(u = model2$df[1], 
            v = NULL, f2 = f2, 
            sig.level = .05, power = .80)
```

---
class: middle
# Categorical Predictors
---
# Modeling Categorical Variables

- Today's Dataset

  - Winter(2016)

    - Are smell words (e.g., *rancid*) rated as more negative/unpleaseant than taste words (e.g., *sweet*)?
    
    - 1 to 9 rating scale
   
```{r}
library(tidyverse)
senses<- read_csv("data/winter_2016_senses_valence.csv")

senses_filt <- senses %>%
  filter(Modality=="Taste" | Modality=="Smell")
```
---
# Linear Model

<br>
<br>

$$\red{Y_i} = \beta_0 + \beta_1 \blue{X_i} + \green{\varepsilon_i}$$

--

- So far .blue[predictor variable] has been continuous

--

- We can also use linear modeling for categorical variables

---
# Categorical Variables

- Terminology

  - *Factor*: a variable with a fix set of categories
  
  - *Levels*: The individual categories within a factor

--

- In our dataset, what is the factor and what are its levels?
---
# Linear Modeling and t-tests/ANOVAs 

What do we do in linear modeling?

- Fit a line (least squares method)

```{r, echo=FALSE, fig.align='center', out.width="100%"}

some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = predict.lm(lm(Y~X))) %>%
  mutate(Y_error = Y - Y_pred)

ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method='lm', se=FALSE)
```

---
# Linear Modeling and *t*-tests/ANOVAs 

Within a t-test/ANOVA framework we want to know if means differ between groups 

```{r, eval=FALSE}
t.test() # test mean differences
```


```{r, echo=FALSE, fig.align='center', out.width="60%"}
senses_filt %>%
  ggplot(aes(x=Modality, y = Val)) + 
   geom_point2(size = 4, alpha = 0.3) + 
   stat_summary(fun.y = mean, color = "black", geom ="point", aes(group = 1), size = 9,
                   show.legend = FALSE, colour="red")
```
---
# Linear Modeling and t-tests/ANOVAs 

- First, calculate $SS_{total}$

<br>
<br>

.pull-left[
```{r, echo=FALSE, fig.align='center', out.width="100%"}

some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = mean(Y)) %>%
  mutate(Y_error = Y - Y_pred)

(total_plot <- ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_line(aes(y=Y_pred), color='black')+
  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+
  geom_rect(aes(ymin=Y, 
                ymax=Y_pred, 
                xmin=X,
                xmax=X+Y_error), 
            alpha = .2)+
  coord_cartesian(xlim=c(0,10),
                  ylim=c(0,10))+
  ggtitle("SS Total")
  )

```
]
.pull-left[
```{r, echo=FALSE, fig.align='center', out.width="100%"}
# mean is 5.69
senses_filt %>%
  ggplot(aes(x=Modality, y = Val)) + 
   geom_point2(size = 4, alpha = 0.3) + 
   stat_summary(fun.y = mean, color = "black", geom ="point", aes(group = 1), size = 9,
                   show.legend = FALSE, colour="red") + 
  geom_hline(yintercept = 5.69, size=4) + 
  ggtitle("SS_Total Two Means: Observed - Grand Mean")
```
]
---
# Fit a line

- Find line using least squares method

  - Mean is best line fit

```{r, echo=FALSE, fig.align='center', out.width="100%"}

some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = predict.lm(lm(Y~X))) %>%
  mutate(Y_error = Y - Y_pred)

ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method='lm', se=FALSE)
```
---
# Fit a line

```{r, echo=FALSE, fig.align='center', out.width="100%"}
library(see)
senses_filt_mean <- senses_filt %>%
  group_by(Modality) %>%
  summarize(mean=mean(Val))

senses_filt %>%
  ggplot(aes(x=Modality, y = Val)) + 
   geom_point2(size = 4, alpha = 0.3) + 
   stat_summary(fun.y = mean, color = "black", geom ="point", aes(group = 1), size = 5,
                   show.legend = FALSE) + 
   stat_summary(fun="mean", geom="segment", mapping=aes(xend=..x.. - 0.25, yend=..y..), color="blue", size=3) + 
  stat_summary(fun="mean", geom="segment", mapping=aes(xend=..x.. + 0.25, yend=..y..), color="blue", size=3)


```
---
# Single Line

- We have two equations 

  - $y = b_0$ = intercept(mean of smell)
  
  - $y = b_0$ = intercept(mean of touch)
  
- How do we get one linear equation?
---
class:middle
# Dummy Coding/Treatment Coding
---
# Dummy Coding/Treatment Coding

- R's default system

    - 0’s and 1’s, with reference level at intercept

    - R does this automatically (0 = whatever comes first alphabetically)

        - Smell = 0 
  
       - Taste = 1

$$\operatorname{Val} = \alpha + \beta_{1}(X_i) + \epsilon$$
<p align="center">
$X_i$ = Indicator of group (0 or 1)
---
# Dummy Coding/Treatment Coding

- Prediction for Smell $X_i =0$

$$\operatorname{Val} = \alpha + \beta_{1}(\operatorname{Modality}_{\operatorname{Taste}}0) + \epsilon$$
$$
\operatorname{\bar{Y}} = \alpha(Smell_{mean})
$$
- Prediction for Taste $X_i =1$

$$\operatorname{Val} = \alpha + \beta_{1}(\operatorname{Modality}_{\operatorname{Taste}}1) + \epsilon$$
$$\operatorname{\bar{Y}{taste}} = \alpha + \beta_{1}(\operatorname{Modality}_{\operatorname{Taste}}1) + \epsilon$$

---
# Dummy Coding By Hand

<br>
<br>
<br>

.pull-left[
```{r}
senses_dum <- senses_filt %>%
  mutate(mod=ifelse(Modality=="Smell", 0, 1))
```
]

.pull-right[

```{r, echo=FALSE, fig.align='center', out.width="100%"}
senses_dum  %>%
  ggplot(aes(x=as.factor(mod), y = Val)) + 
   geom_point2(size = 4, alpha = 0.3) + 
   stat_summary(fun.y = mean, color = "black", geom ="point", aes(group = 1), size = 8,
                   show.legend = FALSE) 
```
]
---
# Categorical Contrast Coding

$$slope=\frac{\mu_{diff}}{run}$$ 

.pull-left[
<br>
<br>
```{r, echo=FALSE, fig.align='center', out.width="80%"}

senses_filt_mean <- senses_filt %>%
  group_by(Modality) %>%
  summarize(mean=mean(Val))

senses_filt %>%
  ggplot(aes(x=Modality, y = Val)) + 
  geom_point(size=3) +
  stat_summary(
    geom = "point",
    fun.y = mean,
    col = "black",
    size = 7,
    fill = "red"
  ) + 
 stat_summary(fun="mean", geom="segment", mapping=aes(xend=..x.. - 0.25, yend=..y..), color="blue", size=3) + 
  stat_summary(fun="mean", geom="segment", mapping=aes(xend=..x.. + 0.25, yend=..y..), color="blue", size=3)
```
]

.pull-right[

```{r, echo=FALSE, fig.align='center', out.width="100%"}


knitr::include_graphics("twomean.JPG")


```
]
---
# Dummy Coded Regression

```{r, eval=FALSE}
lm(Val ~ Modality, data=senses_filt)
```

--

- Intercept: $\bar{Y} = b_0$(Smell) = 5.47

- Slope ($b_1$): Valance of taste words are .337 higher

  - One unit increase (going from 0 to 1; from Smell to Taste) associated with .337 increase in valance scores (mean difference)

- Adding intercept and slope together will give us mean valence of taste words 

---
# Change Reference Level
.pull-left[
```{r}
senses_dum <- senses_filt %>%
  mutate(mod=factor(Modality)) %>% 
  mutate(mod1=relevel(mod, ref="Taste")) # relevel the var 
contrasts(senses_dum$mod1)

contrasts(senses_dum$mod)
```
]
.pull-right[

```{r, eval=FALSE}

lm(Val ~ mod, data=senses_dum)

```

]

---
class:middle 

# Effects Coding/Sum Coding

---
# Effects Coding/Sum Coding

- So far the intercept at 0 has referred to a particular baseline or reference level

--

- Centering (subtracting mean from each value) changes the intercept to correspond to the overall mean

  - While mostly done for continuous variables, you can apply centering to categorical variables
---
# Effects Coding/Sum Coding (.5, -.5)

- Mean of dummy coded variable is = .5

  - Subtract .5 from (0, 1) and we get *+ 0.5* and *-0.5*

- Y intercept is now the grand mean 

$$\frac{\mu_1 + \mu_2}{2}$$
- Slope is still the difference

```{r}

senses_filt_sum <- senses_filt %>%
  mutate(mod_sum=as.factor(Modality), mod_sum_r = as.factor(Modality)) # add a new var to sum code

contrasts(senses_filt_sum$mod_sum) <- c(0.5, -0.5) # change 0 - 1 to +.5 and -.5

```

---
# Effects Coding Results

.pull-left[

```{r, eval=FALSE}
# reg regression
lm(Val ~ Modality, data=senses_filt)
```

]

.pull-right[

```{r, eval=FALSE}
# regression with sum coding (.5 - .5)
lm(Val ~ mod_sum, data=senses_filt_sum)
```
]

--

- Intercept is grand mean (mean of means): $\hat \beta_0=5.64$

- Mean of Smell: $\hat\beta_0+\hat\beta_1\times .5=5.64-0.33(0.5)=5.47$

- Mean of Taste: $\hat\beta_0+\hat\beta_1\times -.5=5.64 + (-0.33)*-0.5=5.80$

---
# Default Sum Coding R behavior

- +1 and -1 

```{r}

senses_filt_sum <- senses_filt_sum %>%
  mutate(mod_sum55=ifelse(mod_sum=="Taste", .5, -.5), mod_sum11 = ifelse(mod_sum=="Taste", 1, -1)) # add a new var to sum code (1, -1)

```

---
# Sum Coding (-1 + 1) Interpretation

.pull-left[

What does this do to our interpretation?

- Intercept is now centered at 0 (grand mean)

- Slope rise is still the same (difference between categories) but: 

  -  Stepping from one category to another (the run) results in overall change of 2 

  - Results are halved
]

.pull-right[
<br>
<br>
```{r, echo=FALSE, fig.align='center', out.width="100%"}

senses_filt_sum %>%
  
  ggplot(aes(x=mod_sum11, y = Val)) + 
  geom_point(size=3) +
  stat_summary(
    geom = "point",
    fun.y = mean,
    col = "black",
    size = 7,
    shape = 24,
    fill = "red"
  ) + 
  geom_vline(xintercept =0) + 
  
  theme_bw(base_size = 18)
```

]
---
# Sum Coding (+1, -1) Model Results

.pull-left[

```{r, eval=FALSE}
lm(Val ~ mod_sum55, data=senses_filt_sum)
```
]
.pull-right[

```{r, eval=FALSE}
lm(Val ~ mod_sum11, data=senses_filt_sum)
```

--

- Intercept is grand mean: $\hat\beta_0$=5.64

- Mean of Smell: 

- Mean of Taste:
]
---
# Why -0.5 and +0.5?

```{r, fig.align='center', out.width="100%", echo=FALSE}

knitr::include_graphics("contrastcodes.bmp")
```
---
# The General Linear F-Test

- Can test overall influence for 2 or more levels of a factor

- We can think about the hypotheses for the overall test being:

  - $H_0$: We cannot predict the dependent variable (over and above a restricted model (only an intercept))
    
  - $H_1$: We can predict the dependent variable (over and above a model with only an intercept)

---
# Restricted Model

$$Y_{ij} = \mu + \epsilon$$
.pull-left[

- Restricted model (Intercept-only): each score $Y_{ij}$ is the result of a single population mean plus random error

$$SS_{error}(R)=\sum(y_i-\bar{y})^2=SS_{total}$$
where:

$y_i$ = observed value
$\bar{y}$ = mean value 
]

.pull-right[
```{r, echo=FALSE, fig.align='center', out.width="80%"}

Y <- c(4,5,2,1,3,7,4,6,3,5,6,9,8,5,7)
myFac <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)
plot(Y, pch=myFac,main="restricted model")
abline(h=mean(Y))
for (i in 1:length(Y)) {
lines(c(i,i), c(Y[i], mean(Y)), lty=2)}


```
]
---
# Full Model

$$Y_{ij} = \mu_j + \epsilon$$
.pull-left[
- Full model (all predictors/levels): each score $Y_ij$ is the result of a different group mean plus random error

$$SSE(F)=\sum(y_{ij}-\hat{y}_{ij})^2=SSE$$
where:

$i$ = Person
$j$ = Group
$y_i$ = Observed value
$\hat{y}$ = Value estimated by regression line
]

.pull-right[
```{r, echo=FALSE,fig.align='center', out.width="80%"}

Y <- c(4,5,2,1,3,7,4,6,3,5,6,9,8,5,7, 4, 5, 6, 7, 4, 5, 6, 7, 4, 5)
IV0 <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5)
plot(Y, pch=IV0, main="full model")
for (j in 1:5) {
w <- which(IV0==j)
lines(c(min(w),max(w)),c(mean(Y[w]),mean(Y[w])))
for (i in 1:length(w)) {
lines(c(w[i],w[i]), c(Y[w[i]], mean(Y[w])), lty=2) }}

```
]
---
# F-ratio

- F-ratio is measure of signal to noise

- Tells us if overall model is significant fit to the data 

   - $H_0$: We cannot predict the dependent variable (over and above a model with only an intercept)
    
    - $H_1$: We can predict the dependent variable (over and above a model with only an intercept)

$$\begin{aligned} &&df_{R} = N - 1\\  &&df_{F} = N - a\\ &&SSE(R)=SS_{total}\\&&SSE(F)=SS_{error}\end{aligned}$$

---
# F-ratio

$$F = \frac{SS_{R}-SS_{F}/{df_{R}-df_{F}} (p-1)}{SS_{F}/df_F(N-p)} = \frac{MS_{model}}{MS_{error}}$$

- If Full =  Restricted , then F=1

- If Full > Restricted, F > 1 

- If Full < Restricted, F < 1

*Degrees of freedom: F(a-1, n - a)*

---
# Plotting Categorical Effects

- Boxplot

```{r, echo=FALSE, fig.align='center', out.width="100%"}

senses_filt %>%
  ggplot(aes(x=Modality, y=Val))+
  geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE)

```
---
# Violin Plots

.pull-left[

```{r, echo=TRUE, fig.align='center', out.width="100%"}

mod=lm(Val ~ Modality, data=senses_filt)

means=modelbased::estimate_means(mod)

d=ggplot(senses_filt, aes(x = Modality, y = Val)) +
  # Add base data
  geom_violin(aes(fill = Modality), color = "white") +
  geom_jitter2(width = 0.05, alpha = 0.5) +

  # Add pointrange and line from means
  geom_line(data = means, aes(y = Mean, group = 1), size = 1) +
  geom_pointrange(
    data = means,
    aes(y = Mean, ymin = CI_low, ymax = CI_high),
    size = 1,
    color = "white"
  ) +
  # Improve colors
  scale_fill_material() +
  theme_modern()
```
]

<br>
<br>

.pull-right[

```{r, echo=FALSE, fig.align='center', out.width="100%"}
d=ggplot(senses_filt, aes(x = Modality, y = Val)) +
  # Add base data
  geom_violin(aes(fill = Modality), color = "white") +
  geom_jitter2(width = 0.05, alpha = 0.5) +

  # Add pointrange and line from means
  geom_line(data = means, aes(y = Mean, group = 1), size = 1) +
  geom_pointrange(
    data = means,
    aes(y = Mean, ymin = CI_low, ymax = CI_high),
    size = 1,
    color = "white"
  ) +
  # Improve colors
  scale_fill_material() +
  theme_modern()

d
```
]

---
# Activity

Mental Health and Drug Use

- CESD: Depressions scores
- unemp: 1=employed 0=unemployed

```{r}
d <- read_csv("https://raw.githubusercontent.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse/master/data/unemployment_pp.csv")
```
---
# Activity

1. Change `unemp` variable to a factor with categorical labels

2. Dummy code the unemployment variable

3. Contrast code the unemployment variable

3. Run `lm` on the dummy coded variable

6. Interpret the output 

7. Use output to extract the means for each group only using the output 

8. Plot the results
---
# Linear Models with Multiple Levels

- So far we have only been looking at two levels

- We easily can extend linear modeling approach to multiple levels

- Let's go back to our sense data

  - Before filtering it down to 2 senses it had 5 senses!

```{r}

senses<- read_csv("data/winter_2016_senses_valence.csv")

glimpse(senses)

``` 
---
Treatment/Dummy Coding: Multilevel Factors

```{r}

lm(Val~Modality, data=senses) %>%
  tidy()

```

- What is going on here? There are only 4 levels, but we actually have 5 levels.

---
# Dummy Coding Extension

.pull-left[
1. Create one fewer dummy codes than levels (K (number of levels)-1)

2. Choose one of your levels as baseline and assign all zeros for this level across each dummy code

3. For first dummy code, assign 1 to first group and 0s for rest of levels

4. For the second dummy code, assign 1 to second group and 0s for rest of levels

5. For third dummy code, assign 1 to third group and 0s for rest of levels

6. For fourth dummy code, assign 1 to fourth group and 0s for rest of levels
]

.pull-right[

```{r, echo=FALSE}

x=contr.treatment(5)

flextable(as.data.frame(x))

```
]

---
# Linear Equation

$$\operatorname{Val} = \alpha + \beta_{1}(\operatorname{Modality}_{\operatorname{Smell}}) + \beta_{2}(\operatorname{Modality}_{\operatorname{Sound}}) + \beta_{3}(\operatorname{Modality}_{\operatorname{Taste}}) + \beta_{4}(\operatorname{Modality}_{\operatorname{Touch}}) + \epsilon$$

```{r, echo=FALSE, fig.align='center', out.width="60%"}
knitr::include_graphics("morethree.JPG")
```

---
# Hello Again Sums of Squares

```{r,echo=FALSE, fig.align='center', out.width="40%"}

knitr::include_graphics("ss_aov.bmp")

```
---
# The General Linear F-Test
 
- We can think about the hypotheses for the overall test being:

$$H_0: b_1 = 0$$
         
$$H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4 = \mu_5$$

$$H_1:  b_1  \neq b_2 \neq b_3 \neq b_4 \neq b_5$$
$$H_1:  \mu_1  \neq \mu_2 \neq \mu_3 \neq \mu_4 \neq \mu_5$$
- *Analysis of Variance (ANOVA)*
---
# Restricted Model

$$Y_{ij} = \mu + \epsilon$$
.pull-left[

- Restricted model (Intercept-only): each score $Y_{ij}$ is the result of a single population mean plus random error

$$SS_{error}(R)=\sum(y_i-\bar{y})^2=SS_{total}$$

where:

$y_i$ = observed value
$\bar{y}$ = mean value 
]

.pull-right[
```{r, echo=FALSE, fig.align='center', out.width="100%"}

Y <- c(4,5,2,1,3,7,4,6,3,5,6,9,8,5,7)
myFac <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3)
plot(Y, pch=myFac,main="restricted model")
abline(h=mean(Y))
for (i in 1:length(Y)) {
lines(c(i,i), c(Y[i], mean(Y)), lty=2)}


```
]
---
# Full Model

$$Y_{ij} = \mu_j + \epsilon$$
.pull-left[
- Full model (all predictors/levels): each score $Y_ij$ is the result of a different group mean plus random error

$$SSE(F)=\sum(y_{ij}-\hat{y}_{ij})^2=SSE$$
where:

$i$ = Person
$j$ = Group
$y_i$ = Observed value
$\hat{y}$ = Value estimated by regression line
]

.pull-right[
```{r, echo=FALSE,fig.align='center', out.width="100%"}

Y <- c(4,5,2,1,3,7,4,6,3,5,6,9,8,5,7, 4, 5, 6, 7, 4, 5, 6, 7, 4, 5)
IV0 <- c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5)
plot(Y, pch=IV0, main="full model")
for (j in 1:5) {
w <- which(IV0==j)
lines(c(min(w),max(w)),c(mean(Y[w]),mean(Y[w])))
for (i in 1:length(w)) {
lines(c(w[i],w[i]), c(Y[w[i]], mean(Y[w])), lty=2) }}

```
]
---
# F-ratio

- F-ratio is measure of signal to noise

$$\begin{aligned} &&df_{R} = N - 1\\  &&df_{F} = N - p\\ &&SSE(R)=SS_{total}\\&&SSE(F)=SS_{error}\end{aligned}$$

---
# F-ratio

$$F = \frac{SS_{R}-SS_{F}/{df_{R}-df_{F}} (p-1)}{SS_{F}/df_F(N-p)} = \frac{MS_{model}}{MS_{error}}$$

- If Full =  Restricted , then F=1

- If Full > Restricted, F > 1 

- If Full < Restricted, F < 1

*Degrees of freedom: F(p-1, n - p)*

---
# ANOVA Table

```{r}
lm(Val~Modality, data=senses) %>%
  parameters::model_parameters()
```
---
# ANOVA Table
```{r}
aov1<-aov(Val~ 1, data=senses)
aov2 <- aov(Val~Modality, data=senses)
#anova(aov1, aov2) compare two models 
aov(Val~Modality, data=senses) %>%
  parameters::model_parameters()
```
---
# Model Comparison Approach vs Traditional Approach to ANOVA

- Traditional formulation of ANOVA asks the same question in a different way:

  - Is the variability between groups (variance due to differences between groups)
greater than expected on the basis of the within-group variability (the variability
within a group) observed, and random sampling of group members?

- Both use sum of squares
- Both use F-statistic
- F = $\frac{MSR}{MSE}$ (Same Mean Squared Error on ANOVA table outputs)
---
# Post-Hoc Comparisons

```{r}
aov_em=aov(Val~Modality, data=senses) # fit ANOVA


aov_em %>%
parameters::model_parameters(.) # print out ANOVA table


```
- The Modality factor is significant. Now what?

---
# Post-Hoc Comparisons

- The best package ever created: `emmeans`

- Allows one to extract marginal means for the model and also test comparisons of interest

---
# Pairwise Tests

- Get means and pairwise comparisons

.pull-left[

```{r, echo=TRUE, eval=TRUE}
# get pairwise tests between all groups

as.data.frame(emmeans::emmeans(aov_em, specs = "Modality")) %>%
flextable()

```
]

.pull-right[
```{r, echo=FALSE}
library(flextable)
# get pairwise tests between all groups
means1 = emmeans::emmeans(aov_em, specs = "Modality")

flextable(as.data.frame(pairs(means1)))
```
]
---
# `afex` ANOVA package

```{r, fig.align='center', out.width="100%"}
library(afex)

one_fit <- aov_ez("Word", "Val", data = senses, between = c("Modality"))

afex_plot(one_fit, x="Modality")

```
---
# Assumptions

- Within linear modeling framework, do normal assumptions checks

```{r, eval=FALSE}

check_model()
```

```{r, echo=FALSE, fig.align='center', out.width="100%", out.height="50%"}
aov(Val~Modality, data=senses) %>%
check_model()
```
---
# Effect Sizes: eta

- $\eta^2$ 

> Interpretation: % of variance explained   

$$\eta^2 = \frac{SS_{model}}{SS_{total}}$$

- $\eta^2$ cannot easily be compared between studies, because the total variability in a study ($SS_{total}$) depends on the design of a study, and increases when additional variables are manipulated


- .01: Small
- .06: Medium
- .14: Large
---
# Effect Sizes:eta

- $\eta_p^2$ 

> Interpretation: % of variance explained for one effect (partailing out others)

$$\eta_p^2 = \frac{SS_{model}}{SS_{model} + SS_{error}}$$
---
# Effect Sizes: eta

- $\eta^2_g$ - Generalized eta-sqaured

$$\frac{SS_{model}}{SS_{model} + SS_{subject} + SS_{error}}$$

---
# Less Biased Effect Size: Omega

- $\omega^2$

$$\omega^2 = \frac{SS_{model} - df_{model}\cdot MS_{error}}{SS_{total} + MS_{error}}$$
- $\omega_p^2$

$$\frac{df_{model} \times (MS_{model} - MS_{error})}{SS_{model} + (N - df_{model}) \times MS_{error}}$$


- .01: Small
- .06: Medium
- .14: Large
---
# Effect Size: Cohen's f

$$\text{Cohen's} f_p =
\sqrt{\frac{\eta^2_p}{1-\eta^2_p}} =
\sqrt{\frac{SS_{effect}}{SS_{error}}}$$

- .14: Small
- .39: Medium
- .59: Large

https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize
---
# Caculate ANOVA Effect Size in R

```{r}
cohens_f(aov_em)
eta_squared(aov_em)
omega_squared(aov_em)
```

---
# Power

- We can also run power analyses for omnibus tests (e.g., number of participants needed  to find a sufficiently powered main effect or interaction)

  - May not sufficiently power one for the smallest desired effect size of interest
  
    - Recommendation: Perform on smallest desired effect size (e.g., mean comparison  while controlling for multiple corrections)
  
- Often complex tests cannot be performed analytically and you must use numerical  methods

- Same approach we have already done!

---
# Power Analysis: ANOVA

- You are planning a reaction-time study involving three groups (k = 3)
- Pilot research & data from literature suggest effect size is medium $f$ = .39
- Suppose you want a power of 0.9
- How many subjects do you need in each sample group?
---
# Power Analysis: ANOVA

```{r}
library(pwr)

pwr.anova.test(k=3,n=NULL,f=.39,sig.level=0.05,power=0.9)

#k = groups
#n= sample size
#es = cohen's f


```

---
# Superpower

- Same RT study

- Pilot research & data from literature suggest population means might be 400, 450  and 500 ms with a sample within-group standard deviation of 100 ms

- Suppose you want a power of 0.9

- How many subjects do you need in each sample group?

```{r}


```
---
# Non-Parametric

.pull-left[
- Kruskal Wallis Test

  - Can be used if assumptions are not met 

      - Extension of Mann-Whitney test
      
    ```{r}
kruskal.test(Val ~ Modality, data=senses)
```
]

.pull-right[
- Welch's F test (W-test)

```{r}
library(onewaytests)

welch.test(Val ~ Modality, data = senses)

```
]

