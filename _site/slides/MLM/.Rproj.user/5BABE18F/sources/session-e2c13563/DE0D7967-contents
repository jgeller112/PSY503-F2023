---
title: "Multilevel Modeing (with R)"
subtitle: "Princeton University"
author: "Jason Geller, PH.D."
date: 'Updated:`r Sys.Date()`'
footer: "PSY 504: Advanced Statistics"
format:
  revealjs:
    theme: psy504.css
    multiplex: true
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
  fig-align: "center"
editor_options: 
  chunk_output_type: inline
---

```{r}
#| echo: false
#| 

library(parameters)
library(effectsize) 
library(papaja)
library(tidyverse)
library(performance)
library(see)
library(equatiomatic)
library(kableExtra)
library(broom)
library(report)
library(emmeans)
library(flextable)
library(ggrain)
library(huxtable)
library(skimr)
library(mediation)
library(JSmediation)
library(MeMoBootR)
library(papaja)
library(tidyverse)
library(processR)
library(broom)
library(ggdist)
library(semPlot)
library(pacman)
library(factoextra)
library(corrplot)
```

## Overview

-   What are multilevel models and why are they awesome?

-   Important terminology

-   Interpretation, estimation, and visualization of multilevel models in R

-   Writing up MLM results

## What we wont have time to talk about ðŸ˜¿

-   Cross-classified models

-   Partial-pooling/shrinkage

-   Growth curve modeling

-   Different types of degrees of freedom

-   Centering

-   Effective sample size

-   Power

-   Cross-level interactions

**If you have specific questions about these things ask me**

## Why multilevel modeling?

-   You might be used to your data looking like this: An independent variable (x) and a dependent variable (y)

    ![](images/simp1.png)

## Why multilevel modeling?

-   However, if we introduce grouping we tell a different story

![](images/simp2.png){fig-align="center"}

## What is multilevel modeling?

-   Simpson's Paradox

![](images/scared-simpsons.gif){fig-align="center"}

## What is multilevel modeling?

![](images/important.png){fig-align="center"}

## What is multilevel modeling?

-   An elaboration on regression to deal with non-independence between data points (i.e., clustered data)

![](images/many_names.JPG){fig-align="center"}

## Hierarchies

![](images/nested.png){fig-align="center"}

-   For now we will focus on data with two levels:
    -   Level one: most basic level of observation
    -   Level two: groups formed from aggregated level-one observation

## Multilevel models are awesome!

::: columns
::: {.column width="50%"}
-   ***Interdependence***
    -   You can model the relationships between cases (regression for repeated observations)
-   ***Missing data***
    -   Uses ML for missing data (partial pooling or shrinkage)
-   ***Power***
    -   Deaggregated data
-   ***Take into account within and between variance***
-   ***Flexibility***
:::

::: {.column width="50%"}
![](images/lego.webp){fig-align="center"}
:::
:::

## Multilevel models

-   When to use them:

    -   Nested designs

    -   Repeated measures

    -   Longitudinal data

-   Why use them:

    -   Captures variance occurring between groups and within groups

-   What they are:

    -   Linear model with extra residuals

# Important Terminology

## Jumping right in

-   Words you hear constantly in MLM Land:

    -   *Fixed effects*
    -   *Random effects*
    -   *Random intercepts*
    -   *Random slopes*

-   What do they all mean?

## Today's data

-   What did you say?

    -   Ps (*N* = 31) listened to clear (NS) and 6 channel vocoded speech (V6)
        -   (https://www.mrc-cbu.cam.ac.uk/personal/matt.davis/vocode/a1_6.wav)

![](images/grand_avg_v6ns.png){fig-align="center"}

## Data

<br>

<br>

::: columns
::: {.column width="50%"}
```{r}
library(tidyverse)
library(lme4) # fit mixed models
library(broom.mixed) # tidy output of mixed models
library(afex) # fit mixed models
library(emmeans) # marginal means
library(ggeffects) # marginal means
eye  <- read_csv("https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/MLM/data/vocoded_pupil.csv")

```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 12
#| fig-height: 8
eye %>% group_by(subject, vocoded) %>% summarise(mean_pupil=mean(mean_pupil)) %>% 
  ggplot(., aes(vocoded, mean_pupil, fill = vocoded)) +
  geom_rain(alpha = .5, rain.side = 'f2x2', id.long.var = "subject") +
  theme_classic() +
  scale_fill_manual(values=c("dodgerblue", "darkorange")) +
  guides(fill = 'none', color = 'none')

```
:::
:::

## Fixed and random effects

-   **Fixed effect**:

    -   Assumed to be constant

    -   Population-level (i.e., average) effects that should persist across experiments

    -   Usually experimental manipulations

    -   Can be continuous or categorical

-   In our data: `vocoded`

## Fixed and random effects

-   **Random effects**:

    -   Assumed to vary at the group/cluster-level

        -   Randomly sampled observations over which you plan to generalize

            -   Participants
            -   Schools
            -   Words
            -   Pictures

-   Can help account for individual variation

-   In our data: `subject`

## Random intercepts

-   Varying starting point (intercept), same slope for each group

    -   ***(1\|participant)***: random intercept for group

        ```{r}
        #| eval: false
        lmer(DV ~ (1|participant))

        ```

![](images/ranint.png){fig-align="center"}

## Random intercepts

-   In a multilevel model, error terms for individual data points are estimated by group

    -   Person-specific deviation from group's predicted outcome

        $$
        y_{ij} = (\beta_{0} + u_{0j})
        $$

![](images/ranint2.png){fig-align="center"}

## Random intercepts - fixed slopes

![](images/ranint3.png){fig-align="center"}

## Random intercepts - random slopes

-   Varying starting point (intercept) **and** slope per group

    -   ***(1+vocoded\|group)***: random intercept and slopes per group

        -   ***Only put a random slope if it changes within cluster/group***

            ```{r}
            #| eval: false

            lmer(DV ~ vocoded + (1+vocoded|participant))

            ```

![](images/ranintslope.png){fig-align="center"}

## Random intercepts - random slope

-   The dotted lines are fixed slopes. The arrows show the added error term for each random slope

$$
y_{ij}=(\beta_{1} + u_{1j})
$$

![](images/randintslope3.png){fig-align="center"}

## All together

![](https://bookdown.org/steve_midway/DAR/images/07_models.png){fig-align="center"}

## Combined multilevel equation

-   Level 1: $$y_{ij} = \beta_{0j} + \beta_{1j}x_{ij} + e_{ij}$$
-   Level 2:

$$ Random Intercept = \beta_{0j} = \beta_{0} + u_{0j}$$

$$Random{slope}=\beta_{1j} = \beta_{1} + u_{1j}$$

-   Mixed Model Equation

$$y_{ij} = (\beta_{0} + u_{0j}) + (\beta_{1} + u_{1j})x_{ij} + e_{ij}$$

# Modeling

## Data organization

-   Data Structure

    -   MLM analysis requires data in long format

```{r}
#| echo: false

head(eye)

```

## Model selection

-   Forward or backward approach

    -   Model 1: Null (unconditional means) model (calculate ICC)

    -   Model 2: full (maximal) model

        -   if non-convergence (pay attention to warning messages):

            -   Try different optimizers [^1]

            -   Deal with random effects

    -   Model 3 (optional): remove fixed effects (e.g., interaction)

[^1]: `afex::all_fit` will test a variety of optimizers and tell you which ones worked

## Model fitting: ML or REML

-   Two flavors of maximum likelihood

    -   Maximum Likelihood (ML or FIML)

        -   Jointly estimate the fixed effects and variance components using all the sample data

        -   Can be used to draw conclusions about fixed and random effects

        -   Issue: Fixed effects are treated as known values when estimating variance components

            -   Results in biased estimates of variance components (especially when sample size is small)

## Model fitting: ML or REML

-   Restricted Maximum Likelihood (REML)

    -   Estimate the variance components using the sample residuals not the sample data

    -   It is conditional on the fixed effects, so it accounts for uncertainty in fixed effects estimates

        -   This results in unbiased estimates of variance components

## Model fitting: ML or REML?

-   Research has not determined one method absolutely superior to the other

-   **REML** (`REML = TRUE`; default in `lmer`) is preferable when:

    -   The number of parameters is large or primary, or

    -   Primary objective is to obtain estimates of the model parameters

-   **ML** (`REML = FALSE`) <u>must</u> be used if you want to compare nested fixed effects models using a likelihood ratio test (e.g., a drop-in-deviance test)

-   For REML, the goodness-of-fit and likelihood ratio tests can only be used to draw conclusions about variance components

## Check assumptions

::: columns
::: {.column width="50%"}
-   Linearity

-   Normality

-   Homoscedasticity
:::

::: {.column width="50%"}
-   Collinearity

-   Outliers
:::
:::

```{r}
#| fig-align: "center"
#| echo: false
library(easystats)
rand_model <- lmer(mean_pupil ~vocoded +(1+vocoded|subject), data = eye)

check_model(rand_model)


```

# Fitting and Interpreting Models

## Linear regression

```{r}

eye_agg <- eye %>%
  group_by(subject, vocoded)%>%
  summarize(mean_pupil=mean(mean_pupil))

lm_model <- lm(mean_pupil ~ vocoded, data = eye_agg)

tidy(lm_model)

```

## Null model (unconditional means)

```{r}
library(lme4) # pop linear modeling package

null_model <- lmer(mean_pupil ~ (1|subject), data = eye)

summary(null_model)

```

## Intraclass correlation (ICC)

-   ICC is a standardized way of expressing how much variance is due to clustering/group

    -   Ranges from 0-1

-   Also, can be interpreted as correlation among observations within cluster/group

-   If ICC is sufficiently low (i.e., $\rho$ \< .1), then you don't have to use MLM! *BUT YOU PROBABLY SHOULD ðŸ™‚*

## Calculating ICC

-   Run baseline (null) model

-   Get intercept variance and residual variance

$$\mathrm{ICC}=\frac{\text { between-group variability }}{\text { between-group variability+within-group variability}}$$

$$
ICC=\frac{\operatorname{Var}\left(u_{0 j}\right)}{\operatorname{Var}\left(u_{0 j}\right)+\operatorname{Var}\left(r_{i j}\right)}=\frac{\tau_{00}}{\tau_{00}+\sigma^{2}}
$$

```{r}
icc <- model_performance(null_model)

icc$ICC
```

## Fixed effects

-   Interpretation same as lm

```{r}

# add the fixed effect of vocode
inter_model <- lmer(mean_pupil ~vocoded+(1|subject), data = eye)

#grab the fixed effects
broom.mixed::tidy(inter_model) %>% filter(effect == "fixed")

```

-   Default behavior is leave out \*p\*-values (Doug Bates doesn't like them)

    -   Install \`lmerTest\` to include \*p\*-values

## Random effects/variance components

-   Tells us how much variability there is around the fixed intercept/slope

    -   How much does the average pupil size change between participants

        ```{r}

        tidy(inter_model) %>% filter(effect == "ran_pars")
        ```

## Visualize random effects

```{r}
#use easystats to grab group variance
random <- estimate_grouplevel(inter_model)

plot(random) +
  theme_lucid()

```

## Maximal model: Fixed effect random intercepts (subject) and slopes (vocoded) model

```{r}

max_model <- lmer(mean_pupil ~vocoded +(1+vocoded|subject), data = eye)

tidy(max_model)

```

## Using `emmeans`

-   Get factor means and contrasts

```{r}

library(emmeans)

emmeans(max_model, specs = "vocoded") # grabs means for each level of modality

emmeans(max_model, specs = "vocoded") %>%
  pairs() # use this to get pariwise compairsons between levels of factors
```

## Maximal models

-   `Keep it maximal`[^2]

    -   Whatever can vary, should vary

        -   include random slopes only if it is a within cluster manipulation

    -   Only when there is convergence issues should you remove terms

-   **Decreases Type 1 error**

[^2]: Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 10.1016/j.jml.2012.11.001. https://doi.org/10.1016/j.jml.2012.11.001

## Visualize random intercepts + slopes

```{r}

random <- estimate_grouplevel(rand_model)

plot(random) +
  theme_lucid()

```

## Random effects/variance components

-   Correlation between random intercepts and slopes

    -   Negative correlation

        -   Higher intercept (for normal speech) less of effect (lower slope)

```{r}
#| echo: false
#| fig-align: "center"
#| 
re = ranef(rand_model)$subject

cor_test(re,  "vocodedV6", "(Intercept)") %>% 
  plot()
```

## Model comparison

-   Can compare models using `anova` function or `test_likelihoodratio` from `easystats`

    -   *Will be refit using ML*

```{r}

#anova(null_model, inter_model, max_model)
test_likelihoodratio(null_model, inter_model, max_model)

```

## LRT

-   For more complex models, use LRT chi-square (drop-in deviance test)

    -   Can be interpreted as main effects and interactions
    -   Use `afex`

```{r}

library(afex)

m <- mixed(mean_pupil ~ 1 + vocoded +  (1+vocoded|subject), data =eye, method = "LRT")

nice(m)
```

# Visualization

## `Easystats`

-   Easily visualize data

```{r}
pupil_data_mean <- eye %>%
  group_by(subject, vocoded) %>%
  summarise(mean_pup=mean(mean_pupil, na.rm=TRUE)) %>% 
  ungroup()

mod_plot <- max_model %>%
  estimate_means("vocoded") %>%
  as.data.frame()

pupil_plot_lmer <- ggplot(pupil_data_mean, aes(x = vocoded, y = mean_pup)) +     
  geom_violinhalf(aes(fill = vocoded), color = "white") +
  geom_jitter2(width = 0.05, alpha = 0.5, size=5) +  # Add pointrange and line from means
  geom_line(aes(y=mean_pup, group=subject))+
  geom_line(data = mod_plot, aes(y = Mean, group = 1), size = 3) +
  geom_pointrange(
    data = mod_plot,
    aes(y = Mean, ymin = CI_low, ymax = CI_high),
    size = 2,
    color = "green"
  ) + 
  # Improve colors
  scale_fill_material() +
  theme_modern() + 
  ggtitle("Pupil Effect", subtitle = "White dots represent model mean and error bars represent 95% CIs. Black dots are group level means for each person")


```

## `Easystats`

```{r}
#| fig-align: "center"

pupil_plot_lmer

```

## `ggeffects`

```{r}
#| fig-align: "center"
#| 
ggemmeans(max_model, terms=c("vocoded"), type="re")%>% plot()
```

## Effect size

-   Highly debated

    -   Report pseudo-$R^2$ for marginal (fixed) and conditional model (random) parts

-   Transform f to $\eta_p^2$ (*when using afex::mixed)*

```{r}
#| eval: false


#get eta for model

F_to_eta2(43.75,1,  52.09)

# get r2 
r2(max_model)
```

## `r2mlm`

```{r}
#| eval: false

# install.packages("devtools")
#devtools::install_github#("mkshaw/r2mlm")

r2mlm::r2mlm(max_model)

```

![](images/r2mlm.png){fig-align="center"}

## Write-up

```{r}
report::report(max_model)
```

## Table

```{r}
modelsummary::modelsummary(max_model, output = "html")
```

## Generalized linear mixed models

-   We can fit most of the models we talked about this semester as a multilevel model

-   Easy to fit Bayesian equivalents as well (using `brms`)

    ```{r}
    #| eval: false
    # poisson
    # binomial
    # negative binomial

    glmer(family="binomial")
    glmer(family="poisson")
    glmer.nb()

    ```

## Shrinkage

![](images/partial-pooling-vs-others-1.png){fig-align="center"}

(https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/)

## Centering

-   You must ensure that the zero value for each predictor is meaningful before running the model

    -   Categorical variables:

        -   Effect-coding or contrast-coding

    -   Continuous Predictors:

In MLM, there are two ways to center, by the grand mean or the group mean

## Group- vs. grand-mean centering

-   Grand-mean centering: $x_{i} - x$

    -   Variable represents each observation's deviation from everyone's norm, regardless of group

-   Group-mean centering: $x_{ij} - x_j$

    -   Variable represents each observation's deviation from their group's norm

## Group- vs. grand-mean centering

-   Level 1 predictors

    -   Grand-mean center

        -   **Include means of level 2**
        -   Allows us to directly test within-group effect

    -   Group-mean center

        -   Level 1 coefficient will always be with within-group effect, regardless of whether the group means are included at Level 2 or not
        -   If level 2 means included, coefficient represents the between-groups effect

## Group mean center in R

```{r}
#| eval: false
library(datawizard) #easystats 

# how to group mean center 
d <- d %>% 
  # Grand mean centering (CMC)
  mutate(iv.gmc = iv-mean(iv)) %>%
  # Person mean centering (more generally, centering within cluster)
  group_by(id) %>% 
  mutate(iv.cm = mean(iv),
         iv.cwc = iv-iv.cm) %>%
  ungroup %>%
  # Grand mean centering of the aggregated variable
  mutate(iv.cmc = iv.cm-mean(iv.cm))
# data wizard way
x <- demean(x, select=c("x"), group="ID") #gets within-group cluster

```
