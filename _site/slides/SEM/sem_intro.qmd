---
title: "Introduction to Structural Equation Modeling in R"
subtitle: "Princeton University"
author: "Jason Geller, PH.D."
date: 'Updated:`r Sys.Date()`'
footer: "PSY 504: Advanced Statistics"
format: 
  revealjs:
    theme: psy504.css
    multiplex: true
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
---

```{r}
#| echo: false
#| 
library(parameters)
library(effectsize) 
library(papaja)
library(tidyverse)
library(performance)
library(see)
library(equatiomatic)
library(kableExtra)
library(broom)
library(report)
library(emmeans)
library(flextable)
library(huxtable)
library(skimr)
library(mediation)
library(JSmediation)
library(MeMoBootR)
library(papaja)
library(tidyverse)
library(processR)
library(broom)
library(ggdist)
library(semPlot)
library(pacman)
```

## Structural equation modeling

-   A broad and extremely versatile multivariate framework

    -   Integration of:

        -   **Path analysis (this week)**

        -   Confirmatory factor analysis (next week)

    -   Test and quantify theories

    -   Measurement error

## Structural equation modeling

::: columns
::: {.column width="50%"}
![](surprise.jpg){fig-align="center"}
:::

::: {.column width="50%"}
-   You already know how to do it!

    -   It is regression on steroids

    -   Model many relationships at once, rather than run single regressions

    -   Model variables that exist (manifest) and those that don't technically exist (latent factors)
:::
:::

## Exogenous vs. endogenous variables

::: columns
::: {.column width="50%"}
<br>

<br>

![](exoendo.png){fig-align="center"}
:::

::: {.column width="50%"}
-   Exogenous
    -   These are synonymous with independent variables
    -   They are thought to be the cause of something
    -   You can find these in a model where the arrow is leaving the variable
    -   Do not have an error term
    -   Changes represented by something else you aren't modeling (like age, gender, etc.)
:::
:::

## Exogenous vs. endogenous variables

::: columns
::: {.column width="50%"}
::: columns
<br>

<br>
:::

![](exoendo.png){fig-align="center"}
:::

::: {.column width="50%"}
-   Endogenous
    -   These are synonymous with dependent variables
    -   They are caused by the exogenous variables
    -   In a model diagram, the arrow will be coming into the variable
    -   Have error terms (assigned automatically by the software)
:::
:::

## Endogenous vs. Exogenous

-   TL;DR

    -   Exogenous: no arrows pointed

    -   Endogenous: pointed errors

        -   A variable can be both (example?)

## Manifest variables

-   Manifest or observed variables

    -   Represented by squares ❏

    -   Measured from participants, business data, or other sources

    -   While most measured variables are continuous, you can use categorical and ordered measures as well

## Latent variables

::: columns
::: {.column width="50%"}
<br>

<br>

![](latent_sem.png){fig-align="center"}
:::

::: {.column width="50%"}
-   Latent variables
    -   Represented by circles ◯

    -   Abstract phenomena you are trying to model

    -   Are not represented by a number in the dataset

    -   Linked to the measured variables

    -   Represented indirectly by those variables
:::
:::

## Important concepts

![](exoendo.png){fig-align="center"}

-   Remember that `Y ~ X + $\epsilon$`

-   Here that is `Endogenous ~ Exogenous + Residual`

    -   Sometimes people call residuals: *disturbances*

## Variances, covariances, and disturbances

## Covariance paths

![](covariancepath.png){fig-align="center"}

-   Double headed arrows
-   Exogenous variables may be correlated with each other
    -   Covariance paths
    -   Variances

## Covariance meaning

![](covarmeaning.png){fig-align="center"}

## Disturbances

![](disturb.png){fig-align="center"}

-   Probably caused by variables that you did not consider

-   Every endogenous variable has a disturbance

## Cheat sheet

![](terms.png){fig-align="center"}

## Path Models

-   Circles are latent (unobserved) variables

-   Squares are manifest (observed) variables

## Path Models

-   Straight arrows are "causal" or directional
    -   Non-standardized solution -\> these are your b or slope values
    -   Standardized solution -\> these are your beta values
-   Curved arrows are non-directional
    -   Non-standardized -\> covariance
    -   Standardized -\> correlation

## Why is it just regression?

-   Each endogenous variable is regressed on all exogenous variables that are connected in the chain that leads directly to it

    ![](semasreg.png){style="<center>" width="603"}

## Model Specification

-   You cannot test all models

    -   Unique solution

-   If not identified, cannot analyze model

-   How is this determined?

## Minimum condition of identification

-   There must be at least as many known values in the model as there are free parameters

-   Free parameters:

    -   All paths, all covariances, all variances, and all disturbances

-   Knowns:

    $$\frac{(K (K+1))}{2}$$ - where k is number of variables in model

## Identification

-   Math example:

    -   10=2x+y

    -   2 = x-y

        -   2 knowns and 2 unknowns

        -   One set of values that can solve the equation

-   Can't test other models

## Identification

::: columns
::: {.column width="50%"}
-   10=2x+y

-   2 = x-y

-   5 = x + 2y

    ![](ident.png)
:::

::: columns
-   Several possible approximate solutions to solve all 3 equations. Several unique but imperfect solutions means multiple models can be tested or compared. You can't evaluate fit without alternatives.
:::
:::

## Model identification and DF

-   We can tell model if identifed by model DFs

    -   Additional pathways you can estimate

    -   Model DF = (known values) - (free paramaters)

    -   If model DF \>=1 you can analyze model

        -   **Over-identified**

## Just-identified model

-   DF = 0

    -   You can analyze the model

        -   But:

            -   Fits data perfectly

            -   No fit indices

-   Multiple regressions are just-identified model

## Estimate DFs

-   Let's play a game

<br>

::: columns
::: {.column width="50%"}
![](https://f3cherokee.com/wp-content/uploads/2020/08/do-you-want-to-play-a-game.jpg.webp){fig-align="center"}
:::

::: {.column width="50%"}
![](https://raw.githubusercontent.com/doomlab/learnSEM/master/vignettes/pictures/lecture_evals.png){fig-align="center"}
:::
:::

???

|     |                                                   |
|-----|---------------------------------------------------|
|     | \- Possible = \$\\frac{4 \\times (4+1)}{2} = 10\$ |
|     | \- Estimated:                                     |
|     | \- 4 regressions                                  |
|     | \- 2 error variances                              |
|     | \- 2 variances                                    |
|     | \- 1 covariance                                   |
|     | \- DF = 10 - 9 = 1                                |

## SEM steps

1.  Specify model
2.  Make sure it is identified
3.  Run the model
4.  Interpret output
5.  Report results

## Model specification

-   Think up all the constructs that are relevant to model
-   Consider how they are connected
-   Justify each path that is present and absent
-   What are the hypothesized relationships?(ivs, dvs, mediators, moderators)

## Example Data

-   What makes someone apply to graduate school?

-   Endogenous

    -   Intent to apply (**intent.to.apply)**
    -   Apply **(application.behaviour)**

-   Exogenous:

    -   Perceived value (**perceived.value)**
    -   External pressure (**external.pressure)**
    -   Perceived control over admission (**perceived.control)**

```{r}
grad <- read.csv(here::here("data", "graduate.school.csv"))

```

## Model

![](model.png){fig-align="center"}

## Data screening

```{r}

correlation::correlation(grad, redundant = FALSE) %>% plot()


```

## Identification

![](check_id.png){fig-align="center"}

## Run SEM in R

::: columns
::: {.column width="50%"}
```{r}
p_load("lavaan", "semPlot")

```

-   Declare equations for every endogenous variable in your model

-   Name paths \*

-   Calculate indirect effects

-   \`\~\` indicates a regression

-   \`\~\~\` indicates a covariance/correlation

-   \`=\~\` indicates a latent variable
:::

::: {.column width="50%"}
```{r}

grad_model = '
intent.to.apply~a*perceived.value+b*external.pressure+c*perceived.control 

    application.behaviour~d*intent.to.apply+perceived.control
    #indirect

    value.through.intent:=a*d
    #indirect
    pressure.through.intent:=b*d  
    control.through.intent:=c*d 

 perceived.control ~~ perceived.value # These are covariance paths
  perceived.control ~~ external.pressure # These are covariance paths
  external.pressure ~~ perceived.value # These are covariance paths
'

```
:::
:::

## Run the model

::: columns
::: {.column width="50%"}
```{r}

library(easystats)

fit <- sem(grad_model, se="bootstrap", data=grad)

```
:::

::: {.column width="50%"}
```{r}
summary(fit, standardize = TRUE, ci=TRUE)

```
:::

## 
:::

## Model fit

![](modfit.png){fig-align="center"}

## Model fit

-   Common to use $\chi^2$ RMSEA, SRMR, CFI
    -   $\chi^2$
        -   Measures how well data fits specified model (goodness of fit)
        -   **Sensitive to large samples**
-   SRMR
    -   Absolute fit statistic
        -   Standardized difference between the observed correlation and the predicted correlation per DF
-   RMSEA
    -   Measures how much worse the data fit the model from the just identified model
-   CFI or Tucker Lewis Index
    -   Incremental fit
        -   Measures how much better the model fits than the null model (all paths=0)

## What to report?

```{r}
library(easystats)
effectsize::interpret(fit)
```

## Reporting Results

```{r}
suppressWarnings(report_performance(fit))
```

## Reporting Results

-   Make reference to a figure with your hypothesized model and parameter estimates and report model fit.

The hypothesized model was tested with path analysis and the estimated model is depicted in Figure 1. The model appeared to have good fit, SRMR = .02, RMSEA= 0, 90% CI \[0, 0.20\], CFI = 1.

## Reporting results

\- Describe significance of the paths

The perceived value of graduate education predicted intentions to apply to grad school, *β* = 0.81, *Z* = 7.21, *p* \< 0.01, but intentions to apply to grad school were not significantly predicted by external pressure to go to grad school, *β* = 0.10, *Z* =  0.97, *p* = 0.33, or perceived control over the outcome of graduate admissions, *β* =-0.13, *Z* = -1.11, *p* =.27. However, intention to go to grad school significantly predicted actually applying, *β* = 0.35, *Z* = 2.94, *p* \< 0.01, and so did perceived  control over the outcome of graduate admissions, *β* = 0.34, *Z* = 2.83, *p* = 0.01.

## Visualize model

![](path_model.png)

## Visualize model

```{r}
library(semPlot)


# Example of plotting the variables in specific locations
locations = matrix(c(0, 0, .5, 0, -.5, .5, -.5, 0, -.5, -.5), ncol=2, byrow=2)
labels = c("Intent\nTo Apply","Application\nBehaviour","Perceived\nValue","External\nPressure","Perceived\nControl")
diagram = semPaths(fit, whatLabels="std", nodeLabels = labels, layout=locations, sizeMan = 12, rotation=2)
```

## Bad fit?

-   Modification (mod) indices

    -   Tell you what the chi-square change would be if you added the path suggested
        -   Potential for HARKING!
        -   Be transparent

    ## Modifications

```{r}
modificationindices(fit) 
```

## Best practices

-   Comparing multiple models

    -   Constraining paths

    -   Assess alternative hypotheses/models

-   Sample size

-   Assumptions

## Best practices

-   Constraining paths

    -   In SEM you can explicitly test hypotheses about the size of specific paths

        -   Constrain a path to certain value

        -   Constrain two paths to be equal

            -   Compare model fit of models with constrained and unconstrained paths

## Constraining paths

::: columns
::: {.column width="50%"}
<br>

<br>

![](contrained.png){fig-align="center"}
:::

::: {.column width="50%"}
```{r}
grad_model_constrained = 
  '
intent.to.apply~a*perceived.value+0*external.pressure+c*perceived.control 

    application.behaviour~d*intent.to.apply+perceived.control
    #indirect

    value.through.intent:=a*d
    #indirect
    control.through.intent:=c*d 

'

grad_analysis_constrained = 
 sem(grad_model_constrained, data=grad, se="bootstrap")

```
:::
:::

## LRT Test

```{r}

anova(fit, grad_analysis_constrained) %>% knitr::kable(., digits = 3)
```

## Nested model comparisons

-   If you can create one model from another by the addition or subtraction of parameters, then it is nested

-   Model A is said to be nested within Model B, if Model B is a more complicated version of Model A

-   Evaluating models

    -   Ensure both fit data well

        -   Report comparative fit indices

-   Use LRT test

    -   anova(model_1, model_2)

## Non-nested model comparisons

![](nonnested.png){fig-align="center"}

-   Use AIC or BIC

    -   $\Delta{BIC}$ (log odds of model with lower BIC)
        -   \>6 strong evidence for model
    -   AIC

    ## Evaluating models

-   Ensure both models fit well

    -   If so compare models with AIC or BIC

    -   If not, choose model that fits

## Write-up: Nested models

We wanted to see if the data fit Ajzen\'s (1985) Theory of planned behavior (\"Unconstrained Model,\" Figure 1) better than a constrained model that posits no relationship between external pressure and intention to apply to graduate school (\"Constrained Model,\" Figure 2). The constrained model fit the data well, SRMR = .03, RMSEA = 0, 90% CI \[0, 0.18\], CFI = 1, AIC = 2000.42, BIC = 2012.98. A Likelihood Ratio test of the two models suggested that the models fit the data equally well, $\chi^2$ (1) = 0.95, p = 0.33. Thus, we trimmed this path in the interest of parsimony. 

## Write-up: Non-nested 

We also compared a non-nested model that considered the strongest pathway of our originally hypothesized model in the context of job opportunities (\"Opportunities Model,\" Figure 3). The opportunitiesmodel had good absolute and relative goodness of fit but the relative badness of fit was poor, SRMR = .05, RMSEA =0.28, 90% CI \[0.13, 0.44\], CFI = 0.96, AIC = 1502.77, BIC = 1519.52. Comparing the Opportunities Model to the Hypothesized Model (Figure 1) using BIC (Kass & Raftery, 1995) reveals that the evidence strongly favors the Opportunities Model, $BIC_{Hypothesized}$ = 2040.69, ΔBIC= 521.

## Practical Issues

-   Sample size: for parameter estimates to be accurate, you should have large samples
-   How many? Hard to say, but often hundreds are necessary
-   http://web.pdx.edu/\~newsomj/semclass/ho_sample%20size.pdf
-   https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4334479/

## Practical Issues

-   Sample Size: The N:q rule
    -   Number of people, *N*
    -   *q* number of estimated parameters
    -   You want the N:q ratio to be 20:1 or greater in a perfect world, 10:1 if you can manage it.

## Assumptions

-   All assumptions for linear models apply to sem
