---
title: "Factor Analysis in R"
subtitle: "Princeton University"
author: "Jason Geller, PH.D."
date: 'Updated:`r Sys.Date()`'
footer: "PSY 504: Advanced Statistics"
format: 
  revealjs:
    theme: psy504.css
    multiplex: true
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
---

```{r}
#| echo: false
#| 
library(parameters)
library(effectsize) 
library(papaja)
library(tidyverse)
library(performance)
library(see)
library(equatiomatic)
library(kableExtra)
library(broom)
library(report)
library(emmeans)
library(flextable)
library(huxtable)
library(skimr)
library(mediation)
library(JSmediation)
library(MeMoBootR)
library(papaja)
library(tidyverse)
library(processR)
library(broom)
library(ggdist)
library(semPlot)
library(pacman)
```

##  How does it work?

-   **Let\'s say we have 6 items in a scale:**

    -   Sleep disturbances (insomnia/hypersomnia)

    -   Suicidal ideation

    -   Lack of interest in normally engaging activities

    -   Racing thoughts

    -   Constant worrying

    -   Nausea

-   **The analysis will \"look\" at the relationships between these items and find that some of them seem to hang together**

-   **In this case, the first 3 items will likely appear to hang together, while the last 3 appear to hang together**

##  How does it work?

**- Let\'s say we have 6 items in a scale:**

-   **Sleep disturbances (insomnia/hypersomnia)**

-   **Suicidal ideation**

-   **Lack of interest in normally engaging activities**

-   Racing thoughts

-   Constant worrying

-   Nausea

\- Some of these could cross-load

-   FA considers this and attributes sleep to both underlying constructs

## Unique and shared variance

-   At broad level, factor analysis is trying to determine whether variables have variability that is driven by an underlying shared factor (shared variance)

-   After, there is leftover variance (unique)

-   Communality: This is **the proportion of each variable's variance that can be explained by the factors**

## Unique and shared variance

![](cfa.png){fig-align="center"}

## Unique and shared variance

-   What we have seen illustrates two goals of factor analysis

    -   Simplify data

        -   6 vars to 2 vars

    -   Identify underlying constructs

        -   Depression and Anxiety

## PCA vs. EFA/CFA

![](PCAvEFA.png){fig-align="center"}

-   

    -   Run *factor analysis* if you assume or wish to test a theoretical model of *latent factors* causing observed variables

    -   Run *principal component analysis* If you want to simply *reduce* your correlated observed variables to a smaller set of important independent composite variables

## FA Steps

1.  Prepare data
    -   Raw data or correlation matrix

    -   Test assumptions
2.  EFA
3.  Decide number of factors
4.  Extraction
5.  Rotate (if needed)
6.  Interpret
7.  Compute factor scores

## Data

::: columns
::: {.column width="50%"}
```{r}
p_load(psych, tidyverse, parameters, foreign, nfactors)

dat <- read.spss("https://stats.idre.ucla.edu/wp-content/uploads/2018/05/SAQ.sav",
                                             to.data.frame=TRUE, use.value.labels = FALSE) %>% 
  select(1:8)
```
:::

::: {.column width="50%"}
Fields: SPSS Anxiety Questionnaire (SAQ-8)

1.  Statistics makes me cry

2.  My friends will think I\'m stupid for not being able to cope with SPSS

3.  Standard deviations excite me

4.  I dream that Pearson is attacking me with correlation coefficients

5.  I don\'t understand statistics

6.  I have little experience with computers

7.  All computers hate me

8.  I have never been good at mathematics
:::
:::

## Data visualization

```{r}

d= correlation::correlation(dat)

s <- summary(d, redundant=TRUE)

plot(s)
# Load the data

```

## Is factor analysis warranted?

-   Bartlett's test

    -   Correlation matrix significantly different from identity matrix (0s)?

    -   Are variables correlated?

-   Kaiser-Meyer-Olkin (KMO)

    -   0-1

    -   Two variables share a common factor they will have small partial correlation (most of variance is explained by common factor so not much left)

    -   Higher is better \>.7

    -   Remove variables \< .5

```{r}

performance::check_factorstructure(dat)

```

## Assumptions

-   Outliers

-   Large sample

    -   \>100

-   Normality

-   No missingness

## Assumptions: Outliers

```{r}

# check outliers
performance::check_outliers(dat)


```

## Fitting Factor Model: \# of factors

Scree plot

-    A plot of the Eigenvalues in order from largest to smallest

-   Look for the elbow (shared variability starting to level off)

    -   Above the elbow is how many components you want

-   ![](scree.png)

## Fitting Factor Model: \# of factors

::: columns
::: {.column width="50%"}
-   Parallel analysis

    -   Run simulations pulling eigenvalues from randomly generated datasets

    -   if eigenvalues \> eigenvalues from random datasets more likely to represent meaningful patterns in the data

    -   More objective and reliable
:::

::: {.column width="50%"}
```{r}

fa.parallel(dat)

```
:::
:::

## Method agreement procedure

```{r}
n_factors(dat) %>% plot()


```

## Extracting factor loadings

-   Once the number of factors are decided the researcher runs another factor analysis to get the loading for each of the factors

    -   ML or principal axis factoring

        -   PA super common and straightforward

            -   An eigen value decomposition of a correlation matrix is done and then the communalities for each variable are estimated by the first n factors. These communalities are entered onto the diagonal and the procedure is repeated until the sum(diag(r)) does not vary.

```{r}
library(parameters)

efa <- psych::fa(data, nfactors = 6, rotate="none", fm="pa") %>%
   model_parameters(sort = TRUE, threshold = "max")

efa

```

## Rotate

-   Sometimes extracted factors make sense, most often they do not

-   Rotation can help

    -   Made more interpretable (understandable) without actually changing the relationships among the variables

        -   Max high loadings and minimize low loadings

## Rotate

::: columns
::: {.column width="50%"}
-   Different types of rotation:

    -   Orthogonal Rotation (Varimax)

        -   This method of rotation prevents the factors from being correlated with each other

        -   Useful if you have factors that should theoretically be unrelated

-   Oblique rotation (Direct Oblimin)

    -   Allows factors to correlate (more common in my own work)
:::

::: {.column width="50%"}

```{r}
#change rotate arg to desired rotation

efa-ob <- psych::fa(data, nfactors = 6, rotate="oblimin", fm="pa")%>% 
   model_parameters(sort = TRUE, threshold = "max") %>%
  print_html()

```
:::
:::

## Rotation

-   When to use which

    -   If 1 factor, dont need to worry

    -   Use direct oblimin if you have a theoritcal reason to believe that your underlying factors should correlate with each other

        -   Factors are uncorrelcted before you rotate,if using direct oblimin, then allowing them to correlate

    -   Use varimax if you prefer to keep factors uncorrelated

## What makes a good factor?

-   Make sense

    -   Loadings on the same factor do not appear to measure completely different things

    -   Easy to interpret

    -   Simple structure

        -   Contains either high or low loadings with few moderatley sized loadings

    -   Lacks cross-loadings

        -   You dont have items that load equally onto more than 1 factor

## Plotting FA

## Table FA

```{r}
source("https://raw.githubusercontent.com/franciscowilhelm/r-collection/master/fa_table.R")


table<- fa_table(efa)
table$ind_table


```

## Confirmatory factor analysi

## Analysis variation
